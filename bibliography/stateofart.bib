@misc{yu2020hyperparameter,
    title={Hyper-Parameter Optimization: A Review of Algorithms and Applications}, 
    author={Tong Yu and Hong Zhu},
    year={2020},
    eprint={2003.05689},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@book{Feurer2019,
    author="Feurer, Matthias
    and Hutter, Frank",
    editor="Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin",
    title="Hyperparameter Optimization",
    bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
    year="2019",
    publisher="Springer International Publishing",
    address="Cham",
    abstract="Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.",
    isbn="978-3-030-05318-5",
    doi="10.1007/978-3-030-05318-5_1",
    url="https://doi.org/10.1007/978-3-030-05318-5_1"
}
@misc{probst2018tunability,
    title={Tunability: Importance of Hyperparameters of Machine Learning Algorithms}, 
    author={Philipp Probst and Bernd Bischl and Anne-Laure Boulesteix},
    year={2018},
    eprint={1802.09596},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@article{ptuningevo2011,
    author = {Eiben, A. and Smit, S.K.},
    year = {2011},
    month = {03},
    pages = {19–31},
    title = {Parameter tuning for configuring and analyzing evolutionary algorithms},
    volume = {1},
    journal = {Swarm and Evolutionary Computation},
    doi = {10.1016/j.swevo.2011.02.001}
}
@article{Bergstra2012,
    author = {Bergstra, James and Bengio, Y.},
    year = {2012},
    month = {03},
    pages = {281-305},
    title = {Random Search for Hyper-Parameter Optimization},
    volume = {13},
    journal = {The Journal of Machine Learning Research}
}
@misc{liashchynskyi2019grid,
    title={Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS}, 
    author={Petro Liashchynskyi and Pavlo Liashchynskyi},
    year={2019},
    eprint={1912.06059},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{JMLR:v13:bergstra12a,
    author  = {James Bergstra and Yoshua Bengio},
    title   = {Random Search for Hyper-Parameter Optimization},
    journal = {Journal of Machine Learning Research},
    year    = {2012},
    volume  = {13},
    number  = {10},
    pages   = {281-305},
    url     = {http://jmlr.org/papers/v13/bergstra12a.html}
}
@misc{dewancker,
    title={Bayesian Optimization Primer}, 
    author={Dewancker, Ian and McCourt, Michael and Clark, Scott},
    year={2015}
}
@inproceedings{NIPS2011_86e8f7ab,
    author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Algorithms for Hyper-Parameter Optimization},
    url = {https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
    volume = {24},
    year = {2011}
}
@article{bengio2000,
    author = {Bengio, Y.},
    year = {2000},
    month = {09},
    pages = {1889-900},
    title = {Gradient-Based Optimization of Hyperparameters},
    volume = {12},
    journal = {Neural computation},
    doi = {10.1162/089976600300015187}
}
@misc{elshawi2019automated,
    title={Automated Machine Learning: State-of-The-Art and Open Challenges},
    author={Radwa Elshawi and Mohamed Maher and Sherif Sakr},
    year={2019},
    eprint={1906.02287},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{jaderberg2017population,
    title={Population Based Training of Neural Networks},
    author={Max Jaderberg and Valentin Dalibard and Simon Osindero and Wojciech M. Czarnecki and Jeff Donahue and Ali Razavi and Oriol Vinyals and Tim Green and Iain Dunning and Karen Simonyan and Chrisantha Fernando},
    year={2017},
    eprint={1711.09846},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{jamieson2015nonstochastic,
    title={Non-stochastic Best Arm Identification and Hyperparameter Optimization},
    author={Kevin Jamieson and Ameet Talwalkar},
    year={2015},
    eprint={1502.07943},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{Katehakis1987TheMB,
    title={The Multi-Armed Bandit Problem: Decomposition and Computation},
    author={M. Katehakis and A. F. Veinott},
    journal={Math. Oper. Res.},
    year={1987},
    volume={12},
    pages={262-268}
}
@misc{li2016hyperband,
    title={Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
    author={Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
    year={2016},
    eprint={1603.06560},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{hazan2017hyperparameter,
    title={Hyperparameter Optimization: A Spectral Approach},
    author={Elad Hazan and Adam Klivans and Yang Yuan},
    year={2017},
    eprint={1706.00764},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
 @online{determined-ai, 
    title={Determined AI Github Repository}, 
    url={https://github.com/determined-ai/determined}, 
    author={Determined-Ai},
    addendum = "accessed: 08.03.2021",
}
 @online{hpbandster, 
    title={HpBandSter a distributed Hyperband implementation on Steroids}, 
    url={https://github.com/automl/HpBandSter}, 
    author={AutoMl},
    addendum = "accessed: 08.03.2021",
}
 @online{hyperopt, 
    title={Hyperopt: Distributed Hyperparameter Optimization}, 
    url={https://github.com/hyperopt/hyperopt}, 
    author={Hyperopt},
    addendum = "accessed: 08.03.2021",
}
 @online{scikit-learn, 
    title={scikit-learn - Machine Learning in Python}, 
    url={https://scikit-learn.org/}, 
    author={scikit-learn},
    addendum = "accessed: 08.03.2021",
}
 @online{auto-sklearn, 
    title={auto-sklearn - an automated machine learning toolkit},
    url={https://github.com/automl/auto-sklearn}, 
    author={AutoMl},
    addendum = "accessed: 08.03.2021",
}
 @online{scikit-optimize, 
    title={Scikit-Optimize - Sequential model-based optimization with a `scipy.optimize` interface}, 
    url={https://github.com/scikit-optimize/scikit-optimize}, 
    author={Scikit-Optimize},
    addendum = "accessed: 08.03.2021",
}
 @online{sagemaker, 
    title={Amazon SageMaker - Machine Learning}, 
    url={https://aws.amazon.com/sagemaker/}, 
    author={Amazon},
    addendum = "accessed: 08.03.2021",
}
 @online{ghypertune, 
    title={AI Platform - Google Cloud}, 
    url={https://cloud.google.com/ai-platform}, 
    author={Google},
    addendum = "accessed: 08.03.2021",
}
 @article{ray,
    title={Tune: A Research Platform for Distributed Model Selection and Training},
    author={Liaw, Richard and Liang, Eric and Nishihara, Robert
            and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
    journal={arXiv preprint arXiv:1807.05118},
    year={2018}
}

@misc{alex2018horovod,
    title={Horovod: fast and easy distributed deep learning in TensorFlow},
    author={Alexander Sergeev and Mike Del Balso},
    year={2018},
    eprint={1802.05799},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{li2018massively,
    title={A System for Massively Parallel Hyperparameter Tuning},
    author={Liam Li and Kevin Jamieson and Afshin Rostamizadeh and Ekaterina Gonina and Moritz Hardt and Benjamin Recht and Ameet Talwalkar},
    year={2018},
    eprint={1810.05934},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@InProceedings{pmlr-v80-falkner18a, 
    title = {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale}, 
    author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank}, 
    booktitle = {Proceedings of the 35th International Conference on Machine Learning}, 
    pages = {1437--1446}, 
    year = {2018}, 
    editor = {Jennifer Dy and Andreas Krause}, 
    volume = {80}, 
    series = {Proceedings of Machine Learning Research}, 
    address = {Stockholmsmässan, Stockholm Sweden}, 
    month = {7}, 
    publisher = {PMLR}, 
    pdf = {http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf}, 
    url = {http://proceedings.mlr.press/v80/falkner18a.html}, 
    abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.} 
}
@phdthesis{10.5555/1571423,
    author = {Yu, Yang},
    title = {Os-Level Virtualization and Its Applications},
    year = {2007},
    isbn = {9780549914075},
    publisher = {State University of New York at Stony Brook},
    address = {USA},
    abstract = {OS-level virtualization is a technology that partitions the operating system to create multiple isolated  Virtual Machines  (VM). An OS-level VM is a virtual execution environment that can be forked instantly from the base operating environment. OS-level virtualization has been widely used to improve security, manageability and availability of today’s complex software environment, with small runtime and resource overhead, and with minimal changes to the existing computing infrastructure. A main challenge with OS-level virtualization is how to achieve strong isolation among VMs that share a common base OS. In this dissertation we study major OS components of Windows NT kernel, and present a  Feather-weight Virtual Machine  (FVM), an OS-level virtualization implementation on Windows platform. The key idea behind FVM is access redirection and copy-on-write, which allow each VM to read from the base environment but write into the VM’s private workspace. In addition, we identify various communication interfaces and confine them in the scope of each individual VM. We demonstrate how to accomplish these tasks to isolate different VMs, and evaluate FVM’s performance overhead and scalability. We present five applications on the FVM framework: secure mobile code execution service, vulnerability assessment support engine, scalable web site testing, shared binary service for application deployment and distributed  Display-Only File Server . To prevent malicious mobile code from compromising desktop’s integrity, we confine the execution of untrusted content inside a VM. To isolate undesirable side effects on production-mode network service during vulnerability scans, we clone the service to be scanned into a VM, and invoke vulnerability scanners on the virtualized service. To identify malicious web sites that exploit browser vulnerabilities, we use a web crawler to access untrusted sites, render their pages with browsers running in VMs, and monitor their execution behaviors. To allow Windows desktop to share binaries that are centrally stored, managed and patched, we launch shared binaries in a special VM whose runtime environment is imported from a central binary server. To protect confidential files in a file server against information theft by insiders, we ensure that file viewing/editing tools run in a client VM, which grants file content display but prevents file content from being saved on the client machine. In this dissertation, we show how to customize the generic FVM framework to accommodate the needs of these applications, and present experimental results that demonstrate their performance and effectiveness.},
    note = {AAI3337611}
}
@misc{foundation_2017, 
    title={A Brief Look at the Roots of Linux Containers}, 
    url={https://www.linuxfoundation.org/blog/a-brief-look-at-the-roots-of-linux-containers/}, 
    journal={Linux Foundation}, 
    publisher={The Linux Foundation}, 
    author={The Linux Foundation}, 
    year={2017}, 
    month={8}
}
@misc{moby_oro, 
     title={Introducing Moby Project: a new open-source project to advance the software containerization movement}, 
     url={https://www.docker.com/blog/introducing-the-moby-project/}, 
     journal={Docker Blog}, 
     author={Oro, David}, 
     year={2021}, 
     month={4}
}
@techreport{stateofcontainers,
    author = {StackRox},
    title = {State of Container and Kubernetes Security},
    publisher = {StackRox},
    year = {2020},
    month = {September},
    url = {https://www.stackrox.com/kubernetes-adoption-security-and-market-share-for-containers/}
}
@misc{kubernetes,
    author = {Kubernetes},
    url={https://github.com/kubernetes/kubernetes/},
    journal={GitHub},
    year={2021}
}
@misc{bauer_2019, 
    title={Docker Containers vs. VMs: Pros and Cons of Containers and Virtual Machines}, 
    url={https://www.backblaze.com/blog/vm-vs-containers/}, 
    journal={Backblaze Blog | Cloud Storage &amp; Cloud Backup}, 
    author={Bauer, Roderick}, 
    year={2019}, 
    month={8}
}
@book{book,
    author = {Audet, Charles and Hare, Warren},
    year = {2017},
    month = {01},
    pages = {},
    title = {Derivative-Free and Blackbox Optimization},
    isbn = {978-3-319-68912-8},
    doi = {10.1007/978-3-319-68913-5}
}
@inproceedings{10.5555/2832581.2832731,
    author = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
    title = {Speeding up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves},
    year = {2015},
    isbn = {9781577357384},
    publisher = {AAAI Press},
    abstract = {Deep neural networks (DNNs) show very strong performance on many machine learning problems, but they are very sensitive to the setting of their hyperparameters. Automated hyperparameter optimization methods have recently been shown to yield settings competitive with those found by human experts, but their widespread adoption is hampered by the fact that they require more computational resources than human experts. Humans have one advantage: when they evaluate a poor hyperparameter setting they can quickly detect (after a few steps of stochastic gradient descent) that the resulting network performs poorly and terminate the corresponding evaluation to save time. In this paper, we mimic the early termination of bad runs using a probabilistic model that extrapolates the performance from the first part of a learning curve. Experiments with a broad range of neural network architectures on various prominent object recognition benchmarks show that our resulting approach speeds up state-of-the-art hyperparameter optimization methods for DNNs roughly twofold, enabling them to find DNN settings that yield better performance than those chosen by human experts.},
    booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
    pages = {3460–3468},
    numpages = {9},
    location = {Buenos Aires, Argentina},
    series = {IJCAI'15}
}