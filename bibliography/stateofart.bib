@misc{yu2020hyperparameter,
    title={Hyper-Parameter Optimization: A Review of Algorithms and Applications}, 
    author={Tong Yu and Hong Zhu},
    year={2020},
    eprint={2003.05689},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@book{Feurer2019,
    author="Feurer, Matthias
    and Hutter, Frank",
    editor="Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin",
    title="Hyperparameter Optimization",
    bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
    year="2019",
    publisher="Springer International Publishing",
    address="Cham",
    abstract="Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.",
    isbn="978-3-030-05318-5",
    doi="10.1007/978-3-030-05318-5_1",
    url="https://doi.org/10.1007/978-3-030-05318-5_1"
}
@misc{probst2018tunability,
    title={Tunability: Importance of Hyperparameters of Machine Learning Algorithms}, 
    author={Philipp Probst and Bernd Bischl and Anne-Laure Boulesteix},
    year={2018},
    eprint={1802.09596},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@article{ptuningevo2011,
    author = {Eiben, A. and Smit, S.K.},
    year = {2011},
    month = {03},
    pages = {19–31},
    title = {Parameter tuning for configuring and analyzing evolutionary algorithms},
    volume = {1},
    journal = {Swarm and Evolutionary Computation},
    doi = {10.1016/j.swevo.2011.02.001}
}
@article{Bergstra2012,
    author = {Bergstra, James and Bengio, Y.},
    year = {2012},
    month = {03},
    pages = {281-305},
    title = {Random Search for Hyper-Parameter Optimization},
    volume = {13},
    journal = {The Journal of Machine Learning Research}
}
@misc{liashchynskyi2019grid,
    title={Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS}, 
    author={Petro Liashchynskyi and Pavlo Liashchynskyi},
    year={2019},
    eprint={1912.06059},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{JMLR:v13:bergstra12a,
    author  = {James Bergstra and Yoshua Bengio},
    title   = {Random Search for Hyper-Parameter Optimization},
    journal = {Journal of Machine Learning Research},
    year    = {2012},
    volume  = {13},
    number  = {10},
    pages   = {281-305},
    url     = {http://jmlr.org/papers/v13/bergstra12a.html}
}
@misc{dewancker,
    title={Bayesian Optimization Primer}, 
    author={Dewancker, Ian and McCourt, Michael and Clark, Scott},
    year={2015}
}
@inproceedings{NIPS2011_86e8f7ab,
    author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Algorithms for Hyper-Parameter Optimization},
    url = {https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
    volume = {24},
    year = {2011}
}
@article{bengio2000,
    author = {Bengio, Y.},
    year = {2000},
    month = {09},
    pages = {1889-900},
    title = {Gradient-Based Optimization of Hyperparameters},
    volume = {12},
    journal = {Neural computation},
    doi = {10.1162/089976600300015187}
}
@misc{elshawi2019automated,
    title={Automated Machine Learning: State-of-The-Art and Open Challenges},
    author={Radwa Elshawi and Mohamed Maher and Sherif Sakr},
    year={2019},
    eprint={1906.02287},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{jaderberg2017population,
    title={Population Based Training of Neural Networks},
    author={Max Jaderberg and Valentin Dalibard and Simon Osindero and Wojciech M. Czarnecki and Jeff Donahue and Ali Razavi and Oriol Vinyals and Tim Green and Iain Dunning and Karen Simonyan and Chrisantha Fernando and Koray Kavukcuoglu},
    year={2017},
    eprint={1711.09846},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{jamieson2015nonstochastic,
    title={Non-stochastic Best Arm Identification and Hyperparameter Optimization},
    author={Kevin Jamieson and Ameet Talwalkar},
    year={2015},
    eprint={1502.07943},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{Katehakis1987TheMB,
    title={The Multi-Armed Bandit Problem: Decomposition and Computation},
    author={M. Katehakis and A. F. Veinott},
    journal={Math. Oper. Res.},
    year={1987},
    volume={12},
    pages={262-268}
}
@misc{li2016hyperband,
    title={Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
    author={Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
    year={2016},
    eprint={1603.06560},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{hazan2017hyperparameter,
    title={Hyperparameter Optimization: A Spectral Approach},
    author={Elad Hazan and Adam Klivans and Yang Yuan},
    year={2017},
    eprint={1706.00764},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
 @online{determined-ai, 
    title={Determined AI Github Repository}, 
    url={https://github.com/determined-ai/determined}, 
    author={Determined-Ai},
    addendum = "accessed: 08.03.2021",
}
 @online{hpbandster, 
    title={HpBandSter a distributed Hyperband implementation on Steroids}, 
    url={https://github.com/automl/HpBandSter}, 
    author={AutoMl},
    addendum = "accessed: 08.03.2021",
}
 @online{hyperopt, 
    title={Hyperopt: Distributed Hyperparameter Optimization}, 
    url={https://github.com/hyperopt/hyperopt}, 
    author={Hyperopt},
    addendum = "accessed: 08.03.2021",
}
 @online{scikit-learn, 
    title={scikit-learn - Machine Learning in Python}, 
    url={https://scikit-learn.org/}, 
    author={scikit-learn},
    addendum = "accessed: 08.03.2021",
}
 @online{auto-sklearn, 
    title={auto-sklearn - an automated machine learning toolkit},
    url={https://github.com/automl/auto-sklearn}, 
    author={AutoMl},
    addendum = "accessed: 08.03.2021",
}
 @online{scikit-optimize, 
    title={Scikit-Optimize - Sequential model-based optimization with a `scipy.optimize` interface}, 
    url={https://github.com/scikit-optimize/scikit-optimize}, 
    author={Scikit-Optimize},
    addendum = "accessed: 08.03.2021",
}
 @online{sagemaker, 
    title={Amazon SageMaker - Machine Learning}, 
    url={https://aws.amazon.com/sagemaker/}, 
    author={Amazon},
    addendum = "accessed: 08.03.2021",
}
 @online{ghypertune, 
    title={AI Platform - Google Cloud}, 
    url={https://cloud.google.com/ai-platform}, 
    author={Google},
    addendum = "accessed: 08.03.2021",
}
 @online{ray, 
    title={Ray - An open source framework that provides a simple, universal API for building distributed applications. }, 
    url={https://ray.io/}, 
    author={{Ray Project}},
    addendum = "accessed: 08.03.2021",
}
@article{liaw2018tune,
    title={Tune: A Research Platform for Distributed Model Selection and Training},
    author={Liaw, Richard and Liang, Eric and Nishihara, Robert
            and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
    journal={arXiv preprint arXiv:1807.05118},
    year={2018}
}

@misc{alex2018horovod,
    title={Horovod: fast and easy distributed deep learning in TensorFlow},
    author={Alexander Sergeev and Mike Del Balso},
    year={2018},
    eprint={1802.05799},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{li2018massively,
    title={A System for Massively Parallel Hyperparameter Tuning},
    author={Liam Li and Kevin Jamieson and Afshin Rostamizadeh and Ekaterina Gonina and Moritz Hardt and Benjamin Recht and Ameet Talwalkar},
    year={2018},
    eprint={1810.05934},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@InProceedings{pmlr-v80-falkner18a, 
title = {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale}, 
author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank}, 
booktitle = {Proceedings of the 35th International Conference on Machine Learning}, 
pages = {1437--1446}, 
year = {2018}, 
editor = {Jennifer Dy and Andreas Krause}, 
volume = {80}, 
series = {Proceedings of Machine Learning Research}, 
address = {Stockholmsmässan, Stockholm Sweden}, 
month = {10--15 Jul}, 
publisher = {PMLR}, 
pdf = {http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf}, 
url = {http://proceedings.mlr.press/v80/falkner18a.html}, 
abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.} 
}