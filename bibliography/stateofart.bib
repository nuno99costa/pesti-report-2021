@misc{yu2020hyperparameter,
      title={Hyper-Parameter Optimization: A Review of Algorithms and Applications}, 
      author={Tong Yu and Hong Zhu},
      year={2020},
      eprint={2003.05689},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@book{Feurer2019,
author="Feurer, Matthias
and Hutter, Frank",
editor="Hutter, Frank
and Kotthoff, Lars
and Vanschoren, Joaquin",
title="Hyperparameter Optimization",
bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
year="2019",
publisher="Springer International Publishing",
address="Cham",
abstract="Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.",
isbn="978-3-030-05318-5",
doi="10.1007/978-3-030-05318-5_1",
url="https://doi.org/10.1007/978-3-030-05318-5_1"
}
@misc{probst2018tunability,
      title={Tunability: Importance of Hyperparameters of Machine Learning Algorithms}, 
      author={Philipp Probst and Bernd Bischl and Anne-Laure Boulesteix},
      year={2018},
      eprint={1802.09596},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{ptuningevo2011,
author = {Eiben, A. and Smit, S.K.},
year = {2011},
month = {03},
pages = {19â€“31},
title = {Parameter tuning for configuring and analyzing evolutionary algorithms},
volume = {1},
journal = {Swarm and Evolutionary Computation},
doi = {10.1016/j.swevo.2011.02.001}
}
@article{Bergstra2012,
author = {Bergstra, James and Bengio, Y.},
year = {2012},
month = {03},
pages = {281-305},
title = {Random Search for Hyper-Parameter Optimization},
volume = {13},
journal = {The Journal of Machine Learning Research}
}
@misc{liashchynskyi2019grid,
      title={Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS}, 
      author={Petro Liashchynskyi and Pavlo Liashchynskyi},
      year={2019},
      eprint={1912.06059},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{JMLR:v13:bergstra12a,
  author  = {James Bergstra and Yoshua Bengio},
  title   = {Random Search for Hyper-Parameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {10},
  pages   = {281-305},
  url     = {http://jmlr.org/papers/v13/bergstra12a.html}
}
@misc{dewancker,
      title={Bayesian Optimization Primer}, 
      author={Dewancker, Ian and McCourt, Michael and Clark, Scott},
      year={2015}
}
@inproceedings{NIPS2011_86e8f7ab,
 author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Algorithms for Hyper-Parameter Optimization},
 url = {https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
 volume = {24},
 year = {2011}
}
@article{bengio2000,
author = {Bengio, Y.},
year = {2000},
month = {09},
pages = {1889-900},
title = {Gradient-Based Optimization of Hyperparameters},
volume = {12},
journal = {Neural computation},
doi = {10.1162/089976600300015187}
}
@misc{elshawi2019automated,
    title={Automated Machine Learning: State-of-The-Art and Open Challenges},
    author={Radwa Elshawi and Mohamed Maher and Sherif Sakr},
    year={2019},
    eprint={1906.02287},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{jaderberg2017population,
    title={Population Based Training of Neural Networks},
    author={Max Jaderberg and Valentin Dalibard and Simon Osindero and Wojciech M. Czarnecki and Jeff Donahue and Ali Razavi and Oriol Vinyals and Tim Green and Iain Dunning and Karen Simonyan and Chrisantha Fernando and Koray Kavukcuoglu},
    year={2017},
    eprint={1711.09846},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{jamieson2015nonstochastic,
    title={Non-stochastic Best Arm Identification and Hyperparameter Optimization},
    author={Kevin Jamieson and Ameet Talwalkar},
    year={2015},
    eprint={1502.07943},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{Katehakis1987TheMB,
  title={The Multi-Armed Bandit Problem: Decomposition and Computation},
  author={M. Katehakis and A. F. Veinott},
  journal={Math. Oper. Res.},
  year={1987},
  volume={12},
  pages={262-268}
}
@misc{li2016hyperband,
    title={Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
    author={Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
    year={2016},
    eprint={1603.06560},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{hazan2017hyperparameter,
    title={Hyperparameter Optimization: A Spectral Approach},
    author={Elad Hazan and Adam Klivans and Yang Yuan},
    year={2017},
    eprint={1706.00764},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
