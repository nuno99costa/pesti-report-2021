@misc{yu2020hyperparameter,
      title={Hyper-Parameter Optimization: A Review of Algorithms and Applications}, 
      author={Tong Yu and Hong Zhu},
      year={2020},
      eprint={2003.05689},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@book{Feurer2019,
author="Feurer, Matthias
and Hutter, Frank",
editor="Hutter, Frank
and Kotthoff, Lars
and Vanschoren, Joaquin",
title="Hyperparameter Optimization",
bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
year="2019",
publisher="Springer International Publishing",
address="Cham",
abstract="Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.",
isbn="978-3-030-05318-5",
doi="10.1007/978-3-030-05318-5_1",
url="https://doi.org/10.1007/978-3-030-05318-5_1"
}
@misc{probst2018tunability,
      title={Tunability: Importance of Hyperparameters of Machine Learning Algorithms}, 
      author={Philipp Probst and Bernd Bischl and Anne-Laure Boulesteix},
      year={2018},
      eprint={1802.09596},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{ptuningevo2011,
author = {Eiben, A. and Smit, S.K.},
year = {2011},
month = {03},
pages = {19â€“31},
title = {Parameter tuning for configuring and analyzing evolutionary algorithms},
volume = {1},
journal = {Swarm and Evolutionary Computation},
doi = {10.1016/j.swevo.2011.02.001}
}
@article{Bergstra2012,
author = {Bergstra, James and Bengio, Y.},
year = {2012},
month = {03},
pages = {281-305},
title = {Random Search for Hyper-Parameter Optimization},
volume = {13},
journal = {The Journal of Machine Learning Research}
}
@misc{liashchynskyi2019grid,
      title={Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS}, 
      author={Petro Liashchynskyi and Pavlo Liashchynskyi},
      year={2019},
      eprint={1912.06059},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{JMLR:v13:bergstra12a,
  author  = {James Bergstra and Yoshua Bengio},
  title   = {Random Search for Hyper-Parameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {10},
  pages   = {281-305},
  url     = {http://jmlr.org/papers/v13/bergstra12a.html}
}
@misc{dewancker,
      title={Bayesian Optimization Primer}, 
      author={Dewancker, Ian and McCourt, Michael and Clark, Scott},
      year={2015}
}
@inproceedings{NIPS2011_86e8f7ab,
 author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Algorithms for Hyper-Parameter Optimization},
 url = {https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
 volume = {24},
 year = {2011}
}
@Book{kochenderfer2019algorithms,
 author = {Kochenderfer, Mykel},
 title = {Algorithms for optimization},
 publisher = {The MIT Press},
 year = {2019},
 address = {Cambridge, Massachusetts},
 isbn = {9780262039420}
 }
@article{bengio2000,
author = {Bengio, Y.},
year = {2000},
month = {09},
pages = {1889-900},
title = {Gradient-Based Optimization of Hyperparameters},
volume = {12},
journal = {Neural computation},
doi = {10.1162/089976600300015187}
}