%!TEX root = ../../../main.tex

\subsection{Related Works}

Automated \acrshort{hpo} approaches can be separated into informed and uninformed. Uninformed methods simply transverse a predefined solution space, selecting the best option, while informed approaches attempt to build on top of previous assumptions to make educated guesses on the best hyper parameter values.

Not all available algorithms were researched, focusing instead on different approaches, usage across the \acrshort{machinel} field and historic significance.

\subsubsection{Uninformed Algorithms}

The 2 most common uninformed methods for \acrshort{hpo} are grid search and random search.

\paragraph{Grid Search}
Traditionally grid search is used for \acrshort{hpo} \parencite{liashchynskyi2019grid}. This method predetermines a definition of hyper parameter possible ranges and tests every hyper parameter combination from the solution space provided (see Figure \ref{grid}).

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			title={Solution space for 2 hyper parameters},
		    xlabel={Hyper parameter $\alpha$},
		    ylabel={Hyper parameter $\beta$},
		    xmin=0, xmax=6,
		    ymin=0, ymax=6,
		    xtick={0,1,2,3,4,5},
		    ytick={0,1,2,3,4,5},
		    ymajorgrids=true,
		    xmajorgrids=true,
		    xticklabels={},
		    yticklabels={}
		]
		\addplot+[
			only marks,
		    mark options={color=lightblue},
		    mark=*,
		    mark size=2.9pt]
		    coordinates {
		    (0.5,0.5)(0.5,1)(0.5,1.5)(0.5,2)(0.5,2.5)(0.5,3)(0.5,3.5)(0.5,4)(0.5,4.5)(0.5,5)(0.5,5.5)
		    (1,0.5)(1,1)(1,1.5)(1,2)(1,2.5)(1,3)(1,3.5)(1,4)(1,4.5)(1,5)(1,5.5)
		    (1.5,0.5)(1.5,1)(1.5,1.5)(1.5,2)(1.5,2.5)(1.5,3)(1.5,3.5)(1.5,4)(1.5,4.5)(1.5,5)(1.5,5.5)
		    (2,0.5)(2,1)(2,1.5)(2,2)(2,2.5)(2,3)(2,3.5)(2,4)(2,4.5)(2,5)(2,5.5)
		    (2.5,0.5)(2.5,1)(2.5,1.5)(2.5,2)(2.5,2.5)(2.5,3)(2.5,3.5)(2.5,4)(2.5,4.5)(2.5,5)(2.5,5.5)
		    (3,0.5)(3,1)(3,1.5)(3,2)(3,2.5)(3,3)(3,3.5)(3,4)(3,4.5)(3,5)(3,5.5)
		    (3.5,0.5)(3.5,1)(3.5,1.5)(3.5,2)(3.5,2.5)(3.5,3)(3.5,3.5)(3.5,4)(3.5,4.5)(3.5,5)(3.5,5.5)
		    (4,0.5)(4,1)(4,1.5)(4,2)(4,2.5)(4,3)(4,3.5)(4,4)(4,4.5)(4,5)(4,5.5)
		    (4.5,0.5)(4.5,1)(4.5,1.5)(4.5,2)(4.5,2.5)(4.5,3)(4.5,3.5)(4.5,4)(4.5,4.5)(4.5,5)(4.5,5.5)
		    (5,0.5)(5,1)(5,1.5)(5,2)(5,2.5)(5,3)(5,3.5)(5,4)(5,4.5)(5,5)(5,5.5)
		    (5.5,0.5)(5.5,1)(5.5,1.5)(5.5,2)(5.5,2.5)(5.5,3)(5.5,3.5)(5.5,4)(5.5,4.5)(5.5,5)(5.5,5.5)
		    };
		\end{axis}
	\end{tikzpicture}
    \caption{Illustration of a grid search. The possible parameters are manually set and the algorithm does an exhaustive search on all the possible combinations.}
    \label{grid}
 \end{figure}

\paragraph{Random Search} This method is similar to grid search, but replaces manually set values with ranges and allows for randomized selection of hyper parameter values (see Figure \ref{random}). This method has been proven to be more efficient than grid search \parencite{JMLR:v13:bergstra12a}.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			title={Solution space for 2 hyper parameters},
		    xlabel={Hyper parameter $\alpha$},
		    ylabel={Hyper parameter $\beta$},
		    xmin=0, xmax=6,
		    ymin=0, ymax=6,
		    xtick={0,1,2,3,4,5},
		    ytick={0,1,2,3,4,5},
		    ymajorgrids=true,
		    xmajorgrids=true,
		    xticklabels={},
		    yticklabels={}
		]
		\addplot+[
			only marks,
		    mark options={color=lightblue},
		    mark=*,
		    mark size=2.9pt]
		    coordinates {
				(1.5,4)(2.5,4.1)(1.4,3)(1.6,3.6)(5.3,2.5)(5.4,2.3)(2.7,3.6)(1.5,5.1)(3.5,1.1)(3.1,5.7)(3.3,3.1)(3.5,1.8)(4.7,5.4)(3.7,2)(4.7,2.2)(2.7,2.4)(3.6,5.6)(2.5,5.9)(1.7,2.7)(4.1,5.9)
		    };
		\end{axis}
	\end{tikzpicture}
    \caption{Illustration of a random search. The hyper parameters values are randomly generated and the algorithm searches all the combinations.}
    \label{random}
 \end{figure}

\subsubsection{Informed Algorithms}

\acrshort{hpo} is an active area of research, with a majority of it focused on informed algorithms. These methods improve over time, by, for example, basing new possible solutions on previous attempts (see evolutionary optimization) or by estimating good candidates based on probabilistic models (see bayesian optimization).

\paragraph{Bayesian Optimization}

Bayesian optimization can be described as sequential model-based optimization, whereas a probabilistic model is updated after every optimization attempt \parencite{dewancker}.

Various probabilistic regression can be used (e.g. Gaussian Processes and Random Forests). One of these regression models, Tree Parzen Estimators, deviates from the standard regression models, which use a predictive distribution $p(y|x)$. Instead, it uses a predictive distribution $p(x|y)$ \parencite{NIPS2011_86e8f7ab}.
 
\paragraph{Simulated Annealing}

This algorithm is based in the annealing process in metallurgy, where metal is heated to a high temperature and then slowly cooled, allowing for atoms to settle into more stable configurations \parencite[p. 128]{kochenderfer2019algorithms}.

\paragraph{Gradient-based Optimization}



\paragraph{Evolutionary Optimization}



\paragraph{Population-based Optimization}



\paragraph{Early Stopping-based Optimization}



\paragraph{HyperBand}



\paragraph{Spectral Optimization}

